{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-08T09:51:43.413759Z",
     "start_time": "2024-02-08T09:51:43.410410Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "# from torchinfo import summary\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c769e2cd4df26bf1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T09:51:54.682949Z",
     "start_time": "2024-02-08T09:51:54.622170Z"
    }
   },
   "outputs": [],
   "source": [
    "# ładujemy tekst\n",
    "with open('pantadeusz.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read().lower() # konwersja na małe litery\n",
    "\n",
    "# tokenizacja\n",
    "tokens = list(text)\n",
    "\n",
    "vocab = sorted(set(tokens))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# stworzenie słownika token -> indeks\n",
    "token_to_index = {token: index for index, token in enumerate(vocab)}\n",
    "index_to_token = {index: token for token, index in token_to_index.items()}\n",
    "\n",
    "# konwersja tokenów na indeksy\n",
    "indexed_tokens = [token_to_index[c] for c in tokens]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6dcec24016dc37c8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T09:52:03.918306Z",
     "start_time": "2024-02-08T09:52:03.863926Z"
    }
   },
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequence, seq_length):\n",
    "        self.sequence = sequence\n",
    "        self.seq_length = seq_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequence) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (torch.tensor(self.sequence[index:index+self.seq_length]),\n",
    "                torch.tensor(self.sequence[index+1:index+self.seq_length+1]))\n",
    "\n",
    "seq_length = 100\n",
    "\n",
    "train_indices, test_indices = train_test_split(list(range(len(indexed_tokens))), test_size=0.1, random_state=42, shuffle=False)\n",
    "\n",
    "\n",
    "# Tworzenie oddzielnych zestawów danych dla treningu i testu\n",
    "train_dataset = CharDataset([indexed_tokens[i] for i in train_indices], seq_length)\n",
    "test_dataset = CharDataset([indexed_tokens[i] for i in test_indices], seq_length)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44ea8af7f38f4d9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CharLSTM(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        out = self.fc(lstm_out)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat.transpose(1, 2), y)\n",
    "        accuracy = (torch.argmax(y_hat, dim=2) == y).float().mean()\n",
    "        self.log('loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log('accuracy', accuracy, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.CrossEntropyLoss()(y_hat.transpose(1, 2), y)\n",
    "        accuracy = (torch.argmax(y_hat, dim=2) == y).float().mean()\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_accuracy', accuracy)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "# Initialize model\n",
    "model = CharLSTM(vocab_size, embed_dim=64, hidden_dim=512, num_layers=2)  # experiment with these parameters\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896f8ed5be2f3e72",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(model)\n",
    "summary(model.cuda(), (seq_length,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dda754a868c4bc0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PyTorch Lightning Trainer setup\n",
    "trainer = pl.Trainer(max_epochs=100, callbacks=[EarlyStopping(monitor='loss')])  # experiment with max epochs\n",
    "trainer.fit(model, train_dataloader)\n",
    "trainer.test(model, test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "17334837af4f72d7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T09:52:23.653488Z",
     "start_time": "2024-02-08T09:52:23.645500Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_text_mod(initial_str, model, length, temperature=1.0):\n",
    "    model.eval()\n",
    "    idxs = [token_to_index[c] for c in initial_str]\n",
    "    input_seq = torch.tensor(idxs).unsqueeze(0)\n",
    "    # input_seq = input_seq.to(device)\n",
    "\n",
    "    generated_sequence = list(idxs)\n",
    "\n",
    "    for _ in range(length):\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq)\n",
    "            distribution = torch.softmax(output[0, -1]/temperature, dim=0).detach().numpy()\n",
    "            next_char_idx = np.random.choice(len(distribution), p=distribution)\n",
    "\n",
    "            generated_sequence.append(next_char_idx)\n",
    "\n",
    "            input_seq = torch.tensor([generated_sequence])\n",
    "    \n",
    "    return ''.join(index_to_token[idx] for idx in generated_sequence)\n",
    "\n",
    "# Generate some text\n",
    "# print(generate_text_mod(\"pan tadeusz poszedl\", model, 200))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af51f5d0949baf42",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_seq20_state_dict.pth')\n",
    "torch.save(model, 'model_seq20.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649ddebfaaa485f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calculate_character_level_accuracy(model, dataloader):\n",
    "    model.eval()  # Przełącz model w tryb ewaluacji\n",
    "    total_correct = 0\n",
    "    total_characters = 0\n",
    "\n",
    "    with torch.no_grad():  # Wyłącz obliczenia gradientów\n",
    "        for batch in dataloader:\n",
    "            sequences, targets = batch\n",
    "            outputs = model(sequences)  # Wygeneruj przewidywania modelu\n",
    "            _, predicted = torch.max(outputs, dim=2)  # Znajdź indeksy największych wartości\n",
    "\n",
    "            total_correct += (predicted == targets).sum().item()  # Sumuj poprawne przewidywania\n",
    "            total_characters += targets.numel()  # Zwiększ całkowitą liczbę znaków\n",
    "\n",
    "    accuracy = total_correct / total_characters  # Oblicz dokładność\n",
    "    return accuracy\n",
    "\n",
    "accuracy = calculate_character_level_accuracy(model, test_dataloader)\n",
    "print(f\"Character-Level Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length: 100\n",
      "Input text (from training dataset):\n",
      "panno święta, co jasnej bronisz częstochowy\n",
      "i w ostrej świecisz bramie! ty, co gród zamkowy\n",
      "nowogród\n",
      "\n",
      "Generated text:\n",
      " w drabinach; dziś życie zawzos zestawicza,\n",
      "bez w istoc, między zakładem nikita\n",
      "jako środkowicz, aż wygrał się zając skrzywdziało».\n",
      "dziś takim ujedwo serca list kąpi,\n",
      "idąc sięm serca wznorze w bliska \n",
      "\n",
      "\n",
      "Input text (from test dataset):\n",
      "tak za dni moich przy wiejskiej zabawie,\n",
      "czytano nieraz pod lipą na trawie\n",
      "pieśń o justynie, powieść\n",
      "\n",
      "Generated text:\n",
      " wmiesz… wziął rad na kończyma:\n",
      "bo przypadnęło: łajć głowę pewnie mu przebaczeni\n",
      "szak wzruszony, trzęs dosyć dał każe,\n",
      "swych miny i tak i gałęzistymi kurki\n",
      "zwierzyła zioła zgodzili, znowu wynarły\n",
      "jako\n",
      "\n",
      "\n",
      "Input text (from outside of dataset):\n",
      "wsiąść do pociągu byle jakiego,\n",
      "nie dbać o bagaż, nie dbać o bilet,\n",
      "ściskając w ręku kamyk zielony,\n",
      "\n",
      "\n",
      "Generated text:\n",
      "i biała rzewa lepszy padł na bauje, był w *rzeczym schwyta,\n",
      "które niknął woje i służy i ostyga;\n",
      "a w soplicowie nie czucie się przyjechał pana zabicia,\n",
      "odgadła we młoduchką; już białą, za obrony,\n",
      "to li\n"
     ]
    }
   ],
   "source": [
    "model = torch.load('model_seq100.pth')\n",
    "\n",
    "print(\"Sequence length:\", seq_length)\n",
    "\n",
    "training_text = \"\"\"Panno święta, co Jasnej bronisz Częstochowy\n",
    "I w Ostrej świecisz Bramie! Ty, co gród zamkowy\n",
    "Nowogródzki ochraniasz z jego wiernym ludem!\n",
    "Jak mnie dziecko do zdrowia powróciłaś cudem\"\"\"\n",
    "\n",
    "training_text = training_text.lower()\n",
    "training_text = training_text[:seq_length]\n",
    "\n",
    "print(\"Input text (from training dataset):\")\n",
    "print(training_text)\n",
    "print(\"\\nGenerated text:\")\n",
    "print(generate_text_mod(training_text, model, 200)[seq_length:])\n",
    "\n",
    "test_text = \"\"\"Tak za dni moich przy wiejskiej zabawie,\n",
    "Czytano nieraz pod lipą na trawie\n",
    "Pieśń o Justynie, powieść o Wiesławie;\n",
    "A przy stoliku drewnianym pan włodarz\"\"\"\n",
    "\n",
    "test_text = test_text.lower()\n",
    "test_text = test_text[:seq_length]\n",
    "\n",
    "print(\"\\n\\nInput text (from test dataset):\")\n",
    "print(test_text)\n",
    "print(\"\\nGenerated text:\")\n",
    "print(generate_text_mod(test_text, model, 200)[seq_length:])\n",
    "\n",
    "outer_text = \"\"\"Wsiąść do pociągu byle jakiego,\n",
    "Nie dbać o bagaż, nie dbać o bilet,\n",
    "Ściskając w ręku kamyk zielony,\n",
    "Patrzeć jak wszystko zostaje w tyle\"\"\"\n",
    "\n",
    "outer_text = outer_text.lower()\n",
    "outer_text = outer_text[:seq_length]\n",
    "\n",
    "print(\"\\n\\nInput text (from outside of dataset):\")\n",
    "print(outer_text)\n",
    "print(\"\\nGenerated text:\")\n",
    "print(generate_text_mod(outer_text, model, 200)[seq_length:])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T09:52:45.337451Z",
     "start_time": "2024-02-08T09:52:29.577592Z"
    }
   },
   "id": "9fe6084fc26c8786",
   "execution_count": 60
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1d7aeb31d37b956b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
